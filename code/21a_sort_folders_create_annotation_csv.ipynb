{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to sort the image files into a before and an after folder, remove unmatched before and after images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "IMG_DIR = os.path.join(DATA_DIR, 'images_ukraine_extracted')\n",
    "IMG_DIR_BEFORE = os.path.join(DATA_DIR, 'images_ukraine_extracted_before')\n",
    "IMG_DIR_AFTER = os.path.join(DATA_DIR, 'images_ukraine_extracted_after')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all event ids from image files\n",
    "img_dirs = os.listdir(IMG_DIR)\n",
    "\n",
    "# move all directories that end with \"after\" to \"images_ukraine_extracted_after\"\n",
    "after_dirs = [img_dir for img_dir in img_dirs if img_dir.endswith('after')]\n",
    "\n",
    "for img_dir in tqdm(after_dirs):\n",
    "    os.rename(os.path.join(IMG_DIR, img_dir), os.path.join(IMG_DIR + '_after', img_dir))\n",
    "\n",
    "# rename IMG_DIR to before\n",
    "os.rename(IMG_DIR, os.path.join(DATA_DIR, 'images_ukraine_extracted_before'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dirs_before = os.listdir(IMG_DIR_BEFORE)\n",
    "image_event_ids_before = [img_dir.split('_')[-2] for img_dir in img_dirs_before]\n",
    "img_dirs_after =  os.listdir(IMG_DIR_AFTER)\n",
    "image_event_ids_after = [img_dir.split('_')[-2] for img_dir in img_dirs_after]\n",
    "\n",
    "# list all ids in before that are not in after\n",
    "ids_not_in_after = [event_id for event_id in image_event_ids_before if event_id not in image_event_ids_after]\n",
    "ids_not_in_before = [event_id for event_id in image_event_ids_after if event_id not in image_event_ids_before]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move unmatched directories to new directory\n",
    "for img_id in tqdm(ids_not_in_after):\n",
    "    # find img dir belonging to event id\n",
    "    img_dir = [img_dir for img_dir in img_dirs_before if img_dir.split(\"_\")[-2]==img_id][0]\n",
    "    os.rename(os.path.join(IMG_DIR_BEFORE, img_dir),\n",
    "              os.path.join(DATA_DIR, 'images_ukraine_unmatched', img_dir))\n",
    "    \n",
    "# now the same for after\n",
    "for img_id in tqdm(ids_not_in_before):\n",
    "    # find img dir belonging to event id\n",
    "    img_dir = [img_dir for img_dir in img_dirs_after if img_dir.split(\"_\")[-2]==img_id][0]\n",
    "    os.rename(os.path.join(IMG_DIR_AFTER, img_dir),\n",
    "              os.path.join(DATA_DIR, 'images_ukraine_unmatched', img_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now strip non-event id stuff from directory names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dirs_before = os.listdir(IMG_DIR_BEFORE)\n",
    "image_event_ids_before = [int(img_dir.split('_')[-2]) for img_dir in img_dirs_before]\n",
    "img_dirs_after =  os.listdir(IMG_DIR_AFTER)\n",
    "image_event_ids_after = [int(img_dir.split('_')[-2]) for img_dir in img_dirs_after]\n",
    "\n",
    "# strip img dirs of stuff that is not the event id\n",
    "for img_dir, event_id in tqdm(zip(img_dirs_before, image_event_ids_before)):\n",
    "    os.rename(os.path.join(IMG_DIR_BEFORE, img_dir),\n",
    "              os.path.join(IMG_DIR_BEFORE, f'{event_id}'))\n",
    "# now same for after\n",
    "for img_dir, event_id in tqdm(zip(img_dirs_after, image_event_ids_after)):\n",
    "    os.rename(os.path.join(IMG_DIR_AFTER, img_dir),\n",
    "              os.path.join(IMG_DIR_AFTER, f'{event_id}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dirs_before = [int(id) for id in os.listdir(IMG_DIR_BEFORE)]\n",
    "\n",
    "event_df = pd.read_csv(os.path.join(DATA_DIR, 'events_data/ACLED_Ukraine_events_timeline.csv'))\n",
    "\n",
    "# keep only rows where timeline_id is in image_event_ids_before\n",
    "event_df_filtered = event_df[event_df['timeline_id'].isin(img_dirs_before)]\n",
    "\n",
    "# left join with places_ukraine.csv\n",
    "places_df = pd.read_csv(os.path.join(DATA_DIR, 'events_data/ukraine_places.csv'))[['location_id', 'location', \"admin1\"]]\n",
    "event_df_filtered = event_df_filtered.merge(places_df, how='left', left_on='location_id', right_on='location_id')\n",
    "\n",
    "# keep only columns that are relevant\n",
    "event_df_filtered = event_df_filtered[['timeline_id', 'location_id', \"location\", \"admin1\", 'event_date', 'overlapping_event',\n",
    "                                       'event', 'any_event', 'cum_attack']]\n",
    "# save to csv\n",
    "event_df_filtered.to_csv(os.path.join(DATA_DIR, 'annotations_ukraine.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create df with image properties anchored on timeline id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "img_dirs_before = os.listdir(IMG_DIR_BEFORE)\n",
    "img_dirs_after =  os.listdir(IMG_DIR_AFTER)\n",
    "\n",
    "df_columns = ['clear_percent', 'cloud_percent', 'shadow_percent', 'snow_ice_percent','visible_percent']\n",
    "\n",
    "# empty df\n",
    "df = pd.DataFrame()\n",
    "counter=0\n",
    "for id in tqdm(img_dirs_before):\n",
    "    counter += 1\n",
    "    with open(os.path.join(IMG_DIR_BEFORE, id, \"files\", \"composite_metadata.json\"), 'r') as file:\n",
    "        before_json = json.load(file)[\"properties\"]\n",
    "    with open(os.path.join(IMG_DIR_AFTER, id, \"files\", \"composite_metadata.json\"), 'r') as file:\n",
    "        after_json = json.load(file)[\"properties\"]\n",
    "    # create new row with the values from the json files, prefix with \"before_\" and \"after_\"\n",
    "    new_row_before = {f'before_{key}': before_json[key] for key in df_columns}\n",
    "    new_row_after = {f'after_{key}': after_json[key] for key in df_columns}\n",
    "    # create new row with the id\n",
    "    new_row = {'id': id}\n",
    "    # concatenate the new rows\n",
    "    new_row.update(new_row_before)\n",
    "    new_row.update(new_row_after)\n",
    "    # append the new row to the df\n",
    "    df = pd.concat([df, pd.DataFrame(new_row, index=[0])], ignore_index=True)\n",
    "\n",
    "# save to csv\n",
    "df.to_csv(os.path.join(DATA_DIR, 'metadata_ukraine.csv'), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
