{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = '../data/annotations_ukraine.csv'\n",
    "BEFORE_PATH = '../data/images_ukraine_extracted_before/'\n",
    "AFTER_PATH = '../data/images_ukraine_extracted_after/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset():\n",
    "    # Instantiate the dataset\n",
    "    dataset = ChangeDetectionDataset(path=CSV_PATH, before_path=BEFORE_PATH, after_path=AFTER_PATH)\n",
    "\n",
    "    # Check the length of the dataset\n",
    "    print(f\"Number of samples in the dataset: {len(dataset)}\")\n",
    "\n",
    "    # Iterate over a few samples\n",
    "    for i in range(3):\n",
    "        sample = dataset[i]\n",
    "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
    "        print(f\"Sample {i}:\")\n",
    "        print(f\"  I1 shape: {I1.shape}, I2 shape: {I2.shape}, Label: {label}\")\n",
    "\n",
    "# Run the test\n",
    "test_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def test_dataloader():\n",
    "    # Instantiate the dataset\n",
    "    dataset = ChangeDetectionDataset(path=CSV_PATH, before_path=BEFORE_PATH, after_path=AFTER_PATH)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "    # Iterate over a few batches\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        I1_batch, I2_batch, labels = batch['I1'], batch['I2'], batch['label']\n",
    "        print(f\"Batch {i}:\")\n",
    "        print(f\"  I1 batch shape: {I1_batch.shape}, I2 batch shape: {I2_batch.shape}, Labels: {labels}\")\n",
    "        if i == 2:  # Limit to a few batches\n",
    "            break\n",
    "\n",
    "# Run the test\n",
    "test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_helpers import ChangeDetectionDataset\n",
    "import numpy as np\n",
    "CSV_PATH = '../data/annotations_ukraine.csv'\n",
    "BEFORE_PATH = '../data/images_ukraine_extracted_before/'\n",
    "AFTER_PATH = '../data/images_ukraine_extracted_after/'\n",
    "\n",
    "\n",
    "def visualize_samples(dataset):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np    \n",
    "    # Visualize a few samples\n",
    "    # get annotations \n",
    "    df = dataset.df\n",
    "    \n",
    "    # set random seed\n",
    "    np.random.seed(7)\n",
    "\n",
    "    indices = np.random.choice(len(df), size=1, replace=False)\n",
    "    for i in indices:\n",
    "        sample = dataset[i]\n",
    "        I1, I2, label = np.squeeze(sample['I1'].numpy()), np.squeeze(sample['I2'].numpy()), sample['label']\n",
    "\n",
    "        # # Normalize the images to [0, 1] range\n",
    "        # I1 = I1/10000\n",
    "        # I2 = I2/10000\n",
    "\n",
    "        # Extract metadata for the title\n",
    "        location = df.iloc[i]['location']\n",
    "        admin1 = df.iloc[i]['admin1']\n",
    "        event_date = df.iloc[i]['event_date']\n",
    "        attack = \"Yes\" if label == 1 else \"No\"\n",
    "        timeline_id = df.iloc[i]['timeline_id']\n",
    "\n",
    "        # Plot the images\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        fig.suptitle(f\"{location}, {admin1}, {event_date}, Attack: {attack}, id: {timeline_id}\", fontsize=14, y=1.05)\n",
    "\n",
    "        axes[0].imshow(np.transpose(I1, (1, 2, 0)))\n",
    "        axes[0].set_title(\"Before\", fontsize=12)\n",
    "        axes[1].imshow(np.transpose(I2, (1, 2, 0)))\n",
    "        axes[1].set_title(\"After\", fontsize=12)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run the visualization\n",
    "dataset = ChangeDetectionDataset(path=CSV_PATH, before_path=BEFORE_PATH,\n",
    "                                     after_path=AFTER_PATH, normalise=False)\n",
    "\n",
    "visualize_samples(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script extracts the dimensions of all images and writes them to a csv. That way I can identify problematic images and set the minimum dimensions for centre cropping. I then manually inspect the csvs, noted down the ids that belong to faulty images and deleted with programatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tifffile\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Base directory containing the image folders\n",
    "base_dir = \"../data/images_ukraine_extracted_after\"\n",
    "\n",
    "def get_image_info(tiff_path):\n",
    "    \"\"\"Extract image ID and dimensions using tifffile\"\"\"\n",
    "    try:\n",
    "        image_id = Path(tiff_path).parts[-3]\n",
    "        \n",
    "        # Read only metadata with tifffile\n",
    "        with tifffile.TiffFile(tiff_path) as tif:\n",
    "            width = tif.pages[0].imagewidth\n",
    "            height = tif.pages[0].imagelength\n",
    "        \n",
    "        return {\"id\": image_id, \"width\": width, \"height\": height}\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {tiff_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Find all TIFF files matching the pattern\n",
    "    print(\"Finding TIFF files...\")\n",
    "    tiff_files = glob.glob(os.path.join(base_dir, \"**/files/composite.tif\"), recursive=True)\n",
    "    print(f\"Found {len(tiff_files)} TIFF files\")\n",
    "    \n",
    "    # Process files in parallel for speed\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        # Use tqdm for a progress bar\n",
    "        results = list(filter(None, tqdm(\n",
    "            executor.map(get_image_info, tiff_files),\n",
    "            total=len(tiff_files),\n",
    "            desc=\"Processing images\"\n",
    "        )))\n",
    "    \n",
    "    # Create a DataFrame from the results\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nProcessed {len(df)} images successfully\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = os.path.join(base_dir, \"after_image_dimensions.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nResults saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_dimensions = pd.read_csv('../data/before_image_dimensions.csv')\n",
    "after_dimensions = pd.read_csv('../data/after_image_dimensions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "ids_to_exclude = [\n",
    "    1130985, 688190, 1805361, 8460613, 2301444, 2095111, \n",
    "    1069195, 5010964, 634941, 3379814, 2244223, 5187329, \n",
    "    1072836, 5165655\n",
    "]\n",
    "\n",
    "# Iterate over the IDs to exclude\n",
    "for id_to_exclude in ids_to_exclude:\n",
    "    # Construct the paths for the before and after directories\n",
    "    before_dir = os.path.join(BEFORE_PATH, str(id_to_exclude))\n",
    "    after_dir = os.path.join(AFTER_PATH, str(id_to_exclude))\n",
    "    \n",
    "    # Remove the directories if they exist\n",
    "    if os.path.exists(before_dir):\n",
    "        shutil.rmtree(before_dir)\n",
    "        print(f\"Removed directory: {before_dir}\")\n",
    "    if os.path.exists(after_dir):\n",
    "        shutil.rmtree(after_dir)\n",
    "        print(f\"Removed directory: {after_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
