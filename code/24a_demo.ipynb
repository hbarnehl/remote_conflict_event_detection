{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo of Model\n",
    "### Get Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by defining the sample:\n",
    "- 100 positive + 100 negative cases\n",
    "- for each class, half are without prior attacks, half after prior attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from extract_features import batch_extract_features\n",
    "# from torch_helpers import ChangeDetectionDataset\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "annotation_path = DATA_DIR + 'annotations_ukraine.csv'\n",
    "before_dir = DATA_DIR + 'images_ukraine_extracted_before'\n",
    "after_dir = DATA_DIR + 'images_ukraine_extracted_after'\n",
    "\n",
    "annotation_df = pd.read_csv(annotation_path)\n",
    "metadata_df = pd.read_csv(DATA_DIR + 'metadata_ukraine.csv')\n",
    "\n",
    "# join on timeline_id=id\n",
    "annotation_df = annotation_df.merge(metadata_df, left_on='timeline_id', right_on='id')\n",
    "\n",
    "# only clearest rows\n",
    "annotation_df = annotation_df[\n",
    "    (annotation_df['before_clear_percent'] == 100) &\n",
    "    (annotation_df['after_clear_percent'] == 100)\n",
    "]\n",
    "\n",
    "# sample\n",
    "N = 1600\n",
    "\n",
    "# Filter the DataFrame for the required conditions\n",
    "cum_attack_0_event_1 = annotation_df[(annotation_df['cum_attack'] == 1) & (annotation_df['event'] == 1)].sample(n=int(N/4), random_state=42)\n",
    "cum_attack_0_event_0 = annotation_df[(annotation_df['cum_attack'] == 0)].sample(n=int(N/4), random_state=42)\n",
    "cum_attack_gt_0_event_1 = annotation_df[(annotation_df['cum_attack'] > 0) & (annotation_df['event'] == 1)].sample(n=int(N/4), random_state=42)\n",
    "cum_attack_gt_0_event_0 = annotation_df[(annotation_df['cum_attack'] > 0) & (annotation_df['event'] == 0)].sample(n=int(N/4), random_state=42)\n",
    "\n",
    "# Combine the samples into a single DataFrame\n",
    "sample_df = pd.concat([cum_attack_0_event_1, cum_attack_0_event_0, cum_attack_gt_0_event_1, cum_attack_gt_0_event_0])\n",
    "\n",
    "# Shuffle the resulting DataFrame (optional)\n",
    "sample_df = sample_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "# save sample df\n",
    "sample_df.to_csv(DATA_DIR + '../data/demo/demo_sample_ukraine.csv', index=False)\n",
    "\n",
    "sample_ids = sample_df['timeline_id'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then extract features based on embedding encodings from foundation model pretrained on google earth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_features import batch_extract_features\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "annotation_path = DATA_DIR + 'annotations_ukraine.csv'\n",
    "before_dir = DATA_DIR + 'images_ukraine_extracted_before'\n",
    "after_dir = DATA_DIR + 'images_ukraine_extracted_after'\n",
    "\n",
    "batch_extract_features(image_ids=sample_ids, before_dir=before_dir,\n",
    "              after_dir=after_dir, checkpoint_path=DATA_DIR + \"model_weights/vit-b-checkpoint-1599.pth\", \n",
    "              output_dir=DATA_DIR + \"features\", device='cuda', use_merged=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Key Changes and Improvements:\n",
    "    Memory Efficiency: The code processes one image pair at a time and clears CUDA cache after each processing step to handle your 4GB GPU constraint.\n",
    "\n",
    "    Pooling Options: I've included multiple pooling methods to experiment with:\n",
    "        avg: Average pooling of all tokens (except CLS)\n",
    "        max: Max pooling of tokens\n",
    "        cls: Using only the CLS token\n",
    "        all: Concatenating CLS with averaged patch tokens\n",
    "    \n",
    "    Sample Limiting: Added a sample_limit parameter to control how many images to process (200 as you specified).\n",
    "\n",
    "    Visualizations: Added proper evaluation metrics and visualizations including ROC curve, confusion matrix, and feature importance.\n",
    "\n",
    "    Saving Intermediate Results: The features are saved to avoid recomputation during experimentation\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import functions from your existing module\n",
    "def process_image_pair_diff_first(row_data, before_dir, after_dir, pooling_method, device):\n",
    "    \"\"\"Process a single image pair, first computing differences then pooling\"\"\"\n",
    "    image_id, label = row_data\n",
    "    # Build paths\n",
    "    before_path = os.path.join(before_dir, str(image_id) + '.npz')\n",
    "    after_path = os.path.join(after_dir, str(image_id) + '.npz')\n",
    "    \n",
    "    # Skip if files don't exist\n",
    "    if not os.path.exists(before_path) or not os.path.exists(after_path):\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Load features\n",
    "        before_data = np.load(before_path)\n",
    "        after_data = np.load(after_path)\n",
    "        before_features = before_data['features']\n",
    "        after_features = after_data['features']\n",
    "        \n",
    "        # Convert to torch\n",
    "        before_features_torch = torch.tensor(before_features)\n",
    "        after_features_torch = torch.tensor(after_features)\n",
    "        \n",
    "        # Calculate token-level differences first\n",
    "        diff_features_torch = after_features_torch - before_features_torch\n",
    "        \n",
    "        # Now apply pooling to the difference features\n",
    "        if pooling_method == 'avg':\n",
    "            # Average pool all tokens except CLS\n",
    "            pooled_diff = diff_features_torch[:, 1:, :].mean(dim=1).numpy()\n",
    "        elif pooling_method == 'max':\n",
    "            # Max pool all tokens except CLS\n",
    "            pooled_diff = diff_features_torch[:, 1:, :].max(dim=1)[0].numpy()\n",
    "        elif pooling_method == 'cls':\n",
    "            # Use only the CLS token difference\n",
    "            pooled_diff = diff_features_torch[:, 0, :].numpy()\n",
    "        elif pooling_method == 'all':\n",
    "            # Concatenate CLS difference with average of patch token differences\n",
    "            cls_diff = diff_features_torch[:, 0, :].numpy()\n",
    "            avg_diff = diff_features_torch[:, 1:, :].mean(dim=1).numpy()\n",
    "            pooled_diff = np.concatenate([cls_diff, avg_diff], axis=1)\n",
    "        \n",
    "        return (pooled_diff.flatten(), label, image_id)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_image_pair(row_data, before_dir, after_dir, pooling_method, device):\n",
    "    \"\"\"Process a single image pair in parallel\"\"\"\n",
    "    image_id, label = row_data\n",
    "    # Build paths\n",
    "    before_path = os.path.join(before_dir, str(image_id) + '.npz')\n",
    "    after_path = os.path.join(after_dir, str(image_id) + '.npz')\n",
    "    \n",
    "    # Skip if files don't exist\n",
    "    if not os.path.exists(before_path) or not os.path.exists(after_path):\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # Load features (using numpy directly to avoid PyTorch overhead)\n",
    "        before_data = np.load(before_path)\n",
    "        after_data = np.load(after_path)\n",
    "        before_features = before_data['features']\n",
    "        after_features = after_data['features']\n",
    "        \n",
    "        # Convert to torch for pooling\n",
    "        before_features_torch = torch.tensor(before_features)\n",
    "        after_features_torch = torch.tensor(after_features)\n",
    "        \n",
    "        # Apply pooling\n",
    "        if pooling_method == 'avg':\n",
    "            pooled_before = before_features_torch[:, 1:, :].mean(dim=1).numpy()\n",
    "            pooled_after = after_features_torch[:, 1:, :].mean(dim=1).numpy()\n",
    "        elif pooling_method == 'max':\n",
    "            pooled_before = before_features_torch[:, 1:, :].max(dim=1)[0].numpy()\n",
    "            pooled_after = after_features_torch[:, 1:, :].max(dim=1)[0].numpy()\n",
    "        elif pooling_method == 'cls':\n",
    "            pooled_before = before_features_torch[:, 0, :].numpy()\n",
    "            pooled_after = after_features_torch[:, 0, :].numpy()\n",
    "        elif pooling_method == 'all':\n",
    "            cls_before = before_features_torch[:, 0, :].numpy()\n",
    "            avg_before = before_features_torch[:, 1:, :].mean(dim=1).numpy()\n",
    "            pooled_before = np.concatenate([cls_before, avg_before], axis=1)\n",
    "            \n",
    "            cls_after = after_features_torch[:, 0, :].numpy()\n",
    "            avg_after = after_features_torch[:, 1:, :].mean(dim=1).numpy()\n",
    "            pooled_after = np.concatenate([cls_after, avg_after], axis=1)\n",
    "        \n",
    "        # Calculate difference\n",
    "        diff_features = pooled_after - pooled_before\n",
    "        \n",
    "        return (diff_features.flatten(), label, image_id)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_change_detection_dataset(before_dir, after_dir, label_path, diff_first, \n",
    "                               pooling_method, sample_limit=5000, device='cuda'):\n",
    "    \"\"\"\n",
    "    Create a dataset for change detection\n",
    "    \n",
    "    Args:\n",
    "        before_dir: Directory with before images\n",
    "        after_dir: Directory with after images\n",
    "        label_path: Path to CSV with labels\n",
    "        pooling_method: Method for pooling features\n",
    "        sample_limit: Maximum number of samples to process\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        X_diff: Feature differences (properly shaped for model training)\n",
    "        y: Labels\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Create a dataset for change detection with parallel processing\n",
    "    \"\"\"\n",
    "    import multiprocessing\n",
    "    from concurrent.futures import ProcessPoolExecutor\n",
    "    from functools import partial\n",
    "    # Load labels\n",
    "    labels_df = pd.read_csv(label_path)\n",
    "    \n",
    "    # Limit samples if needed\n",
    "    if sample_limit and sample_limit < len(labels_df):\n",
    "        labels_df = labels_df.sample(sample_limit, random_state=42)\n",
    "    \n",
    "    # Create a list of (image_id, label) tuples\n",
    "    image_data = list(zip(labels_df['timeline_id'], labels_df['event']))\n",
    "    \n",
    "\n",
    "    if diff_first:\n",
    "        # Create partial function with fixed arguments\n",
    "        process_func = partial(\n",
    "            process_image_pair_diff_first, \n",
    "            before_dir=before_dir, \n",
    "            after_dir=after_dir, \n",
    "            pooling_method=pooling_method, \n",
    "            device=device\n",
    "        )\n",
    "        n_workers = 8\n",
    "        # Use process pool to handle the processing\n",
    "        with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "            results = list(tqdm(executor.map(process_func, image_data), total=len(image_data)))\n",
    "        \n",
    "    else:\n",
    "        # Create partial function with fixed arguments\n",
    "        process_func = partial(\n",
    "            process_image_pair, \n",
    "            before_dir=before_dir, \n",
    "            after_dir=after_dir, \n",
    "            pooling_method=pooling_method, \n",
    "            device=device\n",
    "        )\n",
    "        n_workers = 8\n",
    "        # Use process pool to handle the processing\n",
    "        with ProcessPoolExecutor(max_workers=n_workers) as executor:\n",
    "            results = list(tqdm(executor.map(process_func, image_data), total=len(image_data)))\n",
    "        \n",
    "    # Filter out None results and separate features, labels, image_ids\n",
    "    valid_results = [r for r in results if r is not None]\n",
    "    X_diff, y, image_ids = zip(*valid_results) if valid_results else ([], [], [])\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X_diff = np.array(X_diff)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"Final X_diff shape: {X_diff.shape}\")\n",
    "    \n",
    "    return X_diff, y, image_ids\n",
    "\n",
    "def train_lasso_logistic_regression(X_train, y_train, C):\n",
    "    \"\"\"\n",
    "    Train a Lasso logistic regression model\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels\n",
    "        C: Regularization parameter (1/lambda)\n",
    "        \n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    # Create and train model\n",
    "    model = LogisticRegression(\n",
    "        penalty='l1',\n",
    "        solver='liblinear',\n",
    "        C=C,\n",
    "        max_iter=100000,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        X_test: Test features\n",
    "        y_test: Test labels\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "    }\n",
    "    \n",
    "    # ROC AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    metrics['auc'] = auc(fpr, tpr)\n",
    "    metrics['fpr'] = fpr\n",
    "    metrics['tpr'] = tpr\n",
    "    \n",
    "    return metrics, y_pred\n",
    "\n",
    "def visualize_results(model, metrics, X_test, y_test, image_ids_test=None):\n",
    "    \"\"\"\n",
    "    Visualize model results\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        metrics: Metrics dictionary\n",
    "        X_test: Test features\n",
    "        y_test: Test labels\n",
    "        image_ids_test: List of image IDs for the test set\n",
    "    \"\"\"\n",
    "    # 1. Plot ROC curve\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(metrics['fpr'], metrics['tpr'], 'b-', label=f'AUC = {metrics[\"auc\"]:.3f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    # 2. Plot confusion matrix\n",
    "    plt.subplot(1, 2, 2)\n",
    "    cm = metrics['confusion_matrix']\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(2)\n",
    "    plt.xticks(tick_marks, ['No Change', 'Change'])\n",
    "    plt.yticks(tick_marks, ['No Change', 'Change'])\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"grey\" if cm[i, j] > thresh else \"red\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Feature importance\n",
    "    if hasattr(model, 'coef_'):\n",
    "        top_n = 20\n",
    "        coef = model.coef_[0]\n",
    "        \n",
    "        # Get indices of features with highest absolute weights\n",
    "        top_indices = np.argsort(np.abs(coef))[-top_n:]\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(top_n), coef[top_indices])\n",
    "        plt.yticks(range(top_n), [f'Feature {i}' for i in top_indices])\n",
    "        plt.xlabel('Weight')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.title(f'Top {top_n} Feature Weights')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def run_change_detection_demo(pooling_method, C, diff_first,\n",
    "                              from_pooling=False, sample_limit=5000):\n",
    "    \"\"\"Complete change detection demo pipeline\"\"\"\n",
    "    # Paths\n",
    "    before_dir = '../data/features/before'\n",
    "    after_dir = '../data/features/after'\n",
    "    label_path = '../data/demo/demo_sample_ukraine.csv'\n",
    "    \n",
    "    # Parameters    \n",
    "    if not from_pooling:\n",
    "        # 1. Create dataset\n",
    "        print(\"Creating dataset...\")\n",
    "        X_diff, y, image_ids = create_change_detection_dataset(\n",
    "            before_dir=before_dir,\n",
    "            after_dir=after_dir,\n",
    "            label_path=label_path,\n",
    "            pooling_method=pooling_method,\n",
    "            sample_limit=sample_limit,\n",
    "            diff_first=diff_first\n",
    "        )\n",
    "        \n",
    "        print(f\"Dataset created with {len(X_diff)} samples\")\n",
    "        print(f\"Feature vector shape: {X_diff.shape}\")\n",
    "        \n",
    "        # 2. Save features to avoid recomputation (optional)\n",
    "        np.save('../data/demo/change_detection_features.npy', X_diff)\n",
    "        np.save('../data/demo/change_detection_labels.npy', y)\n",
    "        np.save('../data/demo/change_detection_image_ids.npy', np.array(image_ids))\n",
    "\n",
    "    if from_pooling:\n",
    "        # Load features\n",
    "        X_diff = np.load('../data/demo/change_detection_features.npy')\n",
    "        y = np.load('../data/demo/change_detection_labels.npy')\n",
    "        image_ids = np.load('../data/demo/change_detection_image_ids.npy')\n",
    "        print(f\"Loaded dataset with {len(X_diff)} samples\")\n",
    "        print(f\"Feature vector shape: {X_diff.shape}\")    \n",
    "    \n",
    "    # 3. Split data\n",
    "    X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(\n",
    "        X_diff, y, image_ids, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # 4. Train model\n",
    "    print(\"Training Lasso Logistic Regression model...\")\n",
    "    model = train_lasso_logistic_regression(X_train, y_train, C=C)\n",
    "    \n",
    "    # save model\n",
    "    import joblib\n",
    "    # set model name\n",
    "    model_name = f'../data/demo/lasso_logistic_model_pool={pooling_method}_n={sample_limit}.pkl'\n",
    "    joblib.dump(model, model_name)\n",
    "\n",
    "    # 5. Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    metrics, y_pred = evaluate_model(model, X_test, y_test)\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"AUC: {metrics['auc']:.4f}\")\n",
    "    \n",
    "    # 6. Visualize results\n",
    "    visualize_results(model, metrics, X_test, y_test, ids_test)\n",
    "    \n",
    "    return model, metrics, X_diff, y, image_ids, y_pred, ids_test\n",
    "\n",
    "# # Run the demo\n",
    "# if __name__ == \"__main__\":\n",
    "#     model, X_diff, y, image_ids = run_change_detection_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Import the demo function\n",
    "def run_grid_search(sample_limit=5000):\n",
    "    \"\"\"\n",
    "    Run a grid search over different hyperparameters:\n",
    "    - pooling_method: 'avg', 'max', 'cls', 'all'\n",
    "    - C: 0.1 to 0.7 in 0.1 steps\n",
    "    - diff_first: True, False\n",
    "    \n",
    "    Optimize by running feature extraction once per (pooling_method, diff_first) combination\n",
    "    \"\"\"\n",
    "    # Define parameters for grid search\n",
    "    pooling_methods = ['avg', 'max', 'cls', 'all']\n",
    "    C_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    diff_first_values = [True, False]\n",
    "    \n",
    "    # Create a dataframe to store results\n",
    "    results = []\n",
    "    \n",
    "    # Create directory for results if it doesn't exist\n",
    "    os.makedirs('../data/demo/grid_search', exist_ok=True)\n",
    "    \n",
    "    # Run grid search\n",
    "    for pooling_method in pooling_methods:\n",
    "        for diff_first in diff_first_values:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Testing pooling_method={pooling_method}, diff_first={diff_first}\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # First run: Extract features (from_pooling=False)\n",
    "            print(f\"Extracting features with pooling_method={pooling_method}, diff_first={diff_first}...\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Use C=0.1 for the initial run, but we'll reuse the features for all C values\n",
    "            model, metrics, X_diff, y, image_ids = run_change_detection_demo(\n",
    "                pooling_method=pooling_method,\n",
    "                C=C_values[0],\n",
    "                diff_first=diff_first,\n",
    "                from_pooling=False,\n",
    "                sample_limit=sample_limit\n",
    "            )\n",
    "            \n",
    "            # Save this configuration's results\n",
    "            end_time = time.time()\n",
    "            results.append({\n",
    "                'pooling_method': pooling_method,\n",
    "                'diff_first': diff_first,\n",
    "                'C': C_values[0],\n",
    "                'precision': metrics['precision'],\n",
    "                'recall': metrics['recall'],\n",
    "                'f1': metrics['f1'],\n",
    "                'auc': metrics['auc'],\n",
    "                'accuracy': metrics['accuracy'],\n",
    "                'time': end_time - start_time\n",
    "            })\n",
    "            \n",
    "            # Now iterate through the remaining C values using the saved features\n",
    "            for C in C_values[1:]:\n",
    "                print(f\"Testing C={C} with pooling_method={pooling_method}, diff_first={diff_first}...\")\n",
    "                start_time = time.time()\n",
    "                \n",
    "                model, metrics, X_diff, y, image_ids = run_change_detection_demo(\n",
    "                    pooling_method=pooling_method,\n",
    "                    C=C,\n",
    "                    diff_first=diff_first,\n",
    "                    from_pooling=True,  # Reuse the extracted features\n",
    "                    sample_limit=sample_limit\n",
    "                )\n",
    "                \n",
    "                # Save this configuration's results\n",
    "                end_time = time.time()\n",
    "                results.append({\n",
    "                    'pooling_method': pooling_method,\n",
    "                    'diff_first': diff_first,\n",
    "                    'C': C,\n",
    "                    'precision': metrics['precision'],\n",
    "                    'recall': metrics['recall'],\n",
    "                    'f1': metrics['f1'],\n",
    "                    'auc': metrics['auc'],\n",
    "                    'accuracy': metrics['accuracy'],\n",
    "                    'time': end_time - start_time\n",
    "                })\n",
    "                \n",
    "            # Save intermediate results after each (pooling_method, diff_first) combination\n",
    "            results_df = pd.DataFrame(results)\n",
    "            results_df.to_csv(f'../data/demo/grid_search/results_intermediate_{pooling_method}_{diff_first}.csv', index=False)\n",
    "    \n",
    "    # Convert to dataframe and save\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('../data/demo/grid_search/grid_search_results.csv', index=False)\n",
    "    \n",
    "    # Find best configuration\n",
    "    best_row = results_df.loc[results_df['precision'].idxmax()]\n",
    "    print(\"\\nBest configuration:\")\n",
    "    print(f\"Pooling Method: {best_row['pooling_method']}\")\n",
    "    print(f\"Diff First: {best_row['diff_first']}\")\n",
    "    print(f\"C: {best_row['C']}\")\n",
    "    print(f\"Precision: {best_row['precision']:.4f}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the grid search with 800 samples (modify as needed)\n",
    "    results = run_grid_search(sample_limit=1600)\n",
    "    \n",
    "    # Print a summary of the top 5 configurations by precision\n",
    "    print(b\"\\nTop 5 configurations by precision:\")\n",
    "    top_results = results.sort_values('precision', ascending=False).head(5)\n",
    "    print(top_results[['pooling_method', 'diff_first', 'C', 'precision']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _, _, y_pred, ids_test = run_change_detection_demo(pooling_method='max',C=0.2, diff_first=True,\n",
    "                          from_pooling=False, sample_limit=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn ids test and ypred into df\n",
    "predicted = pd.DataFrame(data={\"timeline_id\":ids_test, \"pred\":y_pred})\n",
    "\n",
    "# load sample csv\n",
    "sample_df = pd.read_csv(\"/run/media/hennes/SSD_2TB/Dropbox/projects/seminar_paper/data/demo/demo_sample_ukraine.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge sample_df and predicted on timeline_id\n",
    "merged = sample_df.merge(right=predicted, how='right', on='timeline_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['correct'] = (merged['event'] == merged['pred']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group merged by correct and compute mean of cum_attack and show table of most common locations\n",
    "merged.groupby('correct').agg({\"cum_attack\":'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[['first_attack', 'event']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variable that is 1 if cum_attack is 1 and 0 otherwise\n",
    "merged['first_attack'] = merged['cum_attack'].apply(lambda x: 1 if x == 1 else 0)\n",
    "merged.groupby('first_attack').agg({'correct':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
