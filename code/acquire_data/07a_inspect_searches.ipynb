{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from planet_helpers import load_search_files\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "import os \n",
    "import random\n",
    "import csv\n",
    "from shapely.geometry import shape, Polygon\n",
    "from shapely.ops import unary_union\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# initialise logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    filename='../search_results_process.log',  # Log file location\n",
    "    level=logging.ERROR,  # Set the logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Log message format\n",
    ")\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "SEARCH_DIR = DATA_DIR + \"/searches\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "timeline_df = pd.read_csv(DATA_DIR + \"/ACLED_Ukraine_events_timeline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_search_files(SEARCH_DIR, 1, timeline_ids=[\"1807921\"])\n",
    "searches = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatevs = [\"events_10000038\", \"events_10000105\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select random search\n",
    "randint = random.randint(0, 130000)\n",
    "\n",
    "searches = load_search_files(SEARCH_DIR, num_files=1, start_index=randint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = searches[\"results\"]\n",
    "aoi = searches[\"filter\"][\"config\"][0][\"config\"][\"coordinates\"][0]\n",
    "# Create a Polygon geometry\n",
    "polygon = Polygon(aoi)\n",
    "\n",
    "day_minus_5 = datetime.strptime(searches[\"filter\"][\"config\"][1][\"config\"][\"gt\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "event_date = (day_minus_5 + timedelta(days=5)).strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort results by clear percent\n",
    "results_example = sorted(results, key=lambda x: x[\"properties\"].get(\"clear_percent\", 0), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results_dict = find_best(results, event_date, polygon)\n",
    "print(best_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1\n",
    "for r in range(1, min(length+1, 4)):\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print acquired date, then new line, then with indent, print id and clear results for all results sorted by the day they were acquired\n",
    "days = set(result[\"properties\"][\"acquired\"].split(\"T\")[0] for result in results)\n",
    "for day in sorted(days):\n",
    "    print(day)\n",
    "    for result in results:\n",
    "        if day in result[\"properties\"][\"acquired\"]:\n",
    "            print(\"\\t\", result[\"id\"], result[\"properties\"].get(\"clear_percent\", 0))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi = searches[\"filter\"][\"config\"][0][\"config\"][\"coordinates\"][0]\n",
    "# Create a Polygon geometry\n",
    "polygon = Polygon(aoi)\n",
    "\n",
    "# Plot the polygon\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "if isinstance(polygon, Polygon):\n",
    "    x, y = polygon.exterior.xy\n",
    "    ax.fill(x, y, alpha=0.5, fc='red', ec='black', label='Union Geometry')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoms = [x[\"geometry\"] for x in searches[\"results\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dictionaries to Shapely geometry objects\n",
    "shapely_geometries = [shape(obj) for obj in day_result_geoms]\n",
    "\n",
    "# Compute the union of all geometries\n",
    "union_geometry = unary_union(shapely_geometries)\n",
    "\n",
    "# Print the result\n",
    "print(union_geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot each original geometry\n",
    "for geom in shapely_geometries:\n",
    "    if isinstance(geom, Polygon):\n",
    "        x, y = geom.exterior.xy\n",
    "        ax.fill(x, y, alpha=0.1, fc='blue', ec='black', label='Original Geometries')\n",
    "x, y = polygon.exterior.xy\n",
    "ax.fill(x, y, alpha=0.5, fc='red', ec='black', label='Union Geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the union geometry\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "if isinstance(union_geometry, Polygon):\n",
    "    x, y = union_geometry.exterior.xy\n",
    "    ax.fill(x, y, alpha=0.5, fc='red', ec='black', label='Union Geometry')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_round(number, decimals=0):\n",
    "    if number is None:\n",
    "        return None\n",
    "    else:\n",
    "        return round(number, decimals)\n",
    "\n",
    "def append_to_csv(file_path, data, header=None):\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if header and not file_exists:\n",
    "            writer.writerow(header)\n",
    "        writer.writerows(data)\n",
    "\n",
    "def get_combinations(elements):\n",
    "    \"\"\"Generate all non-empty combinations of the given elements sorted by their clear percent value.\"\"\"\n",
    "    combins = []\n",
    "    for r in range(1, min(len(elements) + 1, 4)):\n",
    "        for combo in combinations(elements, r):\n",
    "            combo = sorted(combo, key=lambda x: x[\"properties\"].get(\"clear_percent\", 0), reverse=True)\n",
    "            combins.append(list(combo))\n",
    "    logging.info(f\"\\t\\t Found {len(combins)} combinations\")\n",
    "    return combins\n",
    "\n",
    "def calculate_overlap(geometry, aoi):\n",
    "    \"\"\"Calculate the overlap area between a geometry and the area of interest.\"\"\"\n",
    "    return geometry.intersection(aoi).area / aoi.area\n",
    "\n",
    "def calculate_weighted_clear_percentage(combo, aoi):\n",
    "    \"\"\"Calculate the weighted average of clear percentage for a combination.\"\"\"\n",
    "    visible_area = Polygon()\n",
    "    total_visible_area = 0.0\n",
    "    combo_clear_percentage = 0\n",
    "    weighted_sum = 0\n",
    "    for result in combo:\n",
    "        geom = shape(result[\"geometry\"])\n",
    "        clear_percentage = result[\"properties\"].get(\"clear_percent\", 0)\n",
    "\n",
    "        # Calculate the intersection of the current polygon with the AOI\n",
    "        intersection = geom.intersection(aoi)\n",
    "        \n",
    "        # Calculate the new visible area by subtracting the already visible area\n",
    "        new_visible_area = intersection.difference(visible_area)\n",
    "        \n",
    "        # Update the visible area\n",
    "        visible_area = unary_union([visible_area, new_visible_area])\n",
    "        \n",
    "        # Calculate the area of the new visible part\n",
    "        new_visible_area_size = new_visible_area.area\n",
    "        \n",
    "        # Update the weighted sum and total visible area\n",
    "        weighted_sum += new_visible_area_size * clear_percentage\n",
    "        total_visible_area += new_visible_area_size\n",
    "\n",
    "    combo_clear_percentage = weighted_sum / total_visible_area\n",
    "\n",
    "    return combo_clear_percentage\n",
    "\n",
    "def find_best_combination(day_results, aoi):\n",
    "    \"\"\"Find the best combination of results for a given day.\"\"\"\n",
    "    best_combination = None\n",
    "    best_weighted_average = -1\n",
    "    smallest_size = float('inf')\n",
    "    best_overlap = None\n",
    "\n",
    "    for combo in get_combinations(day_results):\n",
    "        combo_ids = [result[\"id\"] for result in combo]\n",
    "        combo_geoms = [shape(result[\"geometry\"]) for result in combo]\n",
    "        combo_union = unary_union(combo_geoms)\n",
    "        combo_overlap = calculate_overlap(combo_union, aoi)\n",
    "\n",
    "        if combo_overlap > 0.95:\n",
    "            combo_clear_percentage = calculate_weighted_clear_percentage(combo, aoi)\n",
    "\n",
    "            # Check if this combination is better or equally good but smaller\n",
    "            if (combo_clear_percentage > best_weighted_average) or \\\n",
    "               (combo_clear_percentage == best_weighted_average and len(combo) < smallest_size):\n",
    "                best_weighted_average = combo_clear_percentage\n",
    "                best_combination = combo_ids\n",
    "                smallest_size = len(combo)\n",
    "                best_overlap = combo_overlap\n",
    "\n",
    "    return best_combination, best_weighted_average,best_overlap\n",
    "\n",
    "def find_best(results, event_date, aoi):\n",
    "    \"\"\"Find the best image combinations before and after a given event date.\"\"\"\n",
    "    aoi_polygon = Polygon(aoi)\n",
    "    days = set(result[\"properties\"][\"acquired\"].split(\"T\")[0] for result in results)\n",
    "    best_day_results = {}\n",
    "\n",
    "    for day in days:\n",
    "        logging.info(f\"\\t Processing day {day}\")\n",
    "        day_results = [result for result in results if result[\"properties\"][\"acquired\"].split(\"T\")[0] == day]\n",
    "        day_result_geoms = [shape(result[\"geometry\"]) for result in day_results]\n",
    "        union_geometry = unary_union(day_result_geoms)\n",
    "        overlap = calculate_overlap(union_geometry, aoi_polygon)\n",
    "\n",
    "        if overlap > 0.95:\n",
    "            best_combination, best_weighted_average, calculated_overlap = find_best_combination(day_results, aoi_polygon)\n",
    "            \n",
    "            if best_combination:\n",
    "                best_day_results[day] = (best_combination, best_weighted_average, calculated_overlap)\n",
    "\n",
    "    best_before = None\n",
    "    best_after = None\n",
    "    \n",
    "    logging.info(f\"\\t considering {len(best_day_results)} best day results\")\n",
    "\n",
    "    for day in best_day_results:\n",
    "        if day < event_date:\n",
    "            if not best_before or best_day_results[day][1] > best_day_results[best_before][1]:\n",
    "                best_before = day\n",
    "        elif day > event_date:\n",
    "            if not best_after or best_day_results[day][1] > best_day_results[best_after][1]:\n",
    "                best_after = day\n",
    "\n",
    "    best_before_result = best_day_results.get(best_before, (None, None, None))\n",
    "    best_after_result = best_day_results.get(best_after, (None, None, None))\n",
    "\n",
    "    best_results_dict = {\n",
    "        \"best_before\": {\n",
    "            \"date\": best_before,\n",
    "            \"ids\": best_before_result[0],\n",
    "            \"clear_percent\": new_round(best_before_result[1], 0),\n",
    "            \"overlap\": new_round(best_before_result[2], 0)\n",
    "        },\n",
    "        \"best_after\": {\n",
    "            \"date\": best_after,\n",
    "            \"ids\": best_after_result[0],\n",
    "            \"clear_percent\": new_round(best_after_result[1], 0),\n",
    "            \"overlap\": new_round(best_after_result[2], 0)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return best_results_dict\n",
    "\n",
    "def process_data(data):\n",
    "    # Placeholder for data processing logic\n",
    "    # This function should return a list of processed data rows\n",
    "    processed_data = []\n",
    "    for search in data:\n",
    "        try:\n",
    "            search_name = search[\"name\"]\n",
    "            logging.info(f\"Processing search {search_name}\")\n",
    "            search_id = search[\"id\"]\n",
    "            aoi = search[\"filter\"][\"config\"][0][\"config\"][\"coordinates\"][0]\n",
    "            day_minus_5 = datetime.strptime(search[\"filter\"][\"config\"][1][\"config\"][\"gt\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            event_date = (day_minus_5 + timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "            quality_results = [result for result in search[\"results\"] if result[\"properties\"][\"quality_category\"] == \"standard\"]\n",
    "\n",
    "            best_results_dict = find_best(quality_results, event_date, aoi)\n",
    "            processed_data.append([search_name,\n",
    "                                   best_results_dict[\"best_before\"][\"date\"], best_results_dict[\"best_before\"][\"ids\"], best_results_dict[\"best_before\"][\"clear_percent\"], best_results_dict[\"best_before\"][\"overlap\"],\n",
    "                                   best_results_dict[\"best_after\"][\"date\"], best_results_dict[\"best_after\"][\"ids\"], best_results_dict[\"best_after\"][\"clear_percent\"], best_results_dict[\"best_after\"][\"overlap\"]])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing search {search['id']}: {e}\")\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(\n",
    "        filename='../search_results_process.log',  # Log file location\n",
    "        level=logging.INFO,  # Set the logging level\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'  # Log message format\n",
    "    )\n",
    "\n",
    "\n",
    "    DATA_DIR = \"../data\"\n",
    "    SEARCH_DIR = DATA_DIR + \"/searches\"\n",
    "    OUTPUT_CSV = DATA_DIR + \"/best_search_results.csv\"\n",
    "\n",
    "    # check if output_csv exists\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        # load existing CSV file\n",
    "        best_results_df = pd.read_csv(OUTPUT_CSV)\n",
    "        header = None\n",
    "    else:\n",
    "        header = [\"search_id\", \"before_date\", \"before_image_id\", \"before_agg_clear\", \"before_overlap\",\n",
    "                \"after_date\", \"after_image_id\", \"after_agg_clear\", \"after_overlap\"]\n",
    "\n",
    "    # create best matches for each search with columns search_id, before_image_id, before_cloud_cover/clarity etc., after_image_id, after_cloud_cover\n",
    "    start_index = 0\n",
    "    chunk_size = 10000\n",
    "\n",
    "    while True:\n",
    "        # Load a chunk of files\n",
    "        json_data = load_search_files(SEARCH_DIR, chunk_size, start_index=start_index)\n",
    "        if not json_data:\n",
    "            print(\"No more data to process\")\n",
    "            break\n",
    "        \n",
    "        # remove any search that has already been processed\n",
    "        if header is None:\n",
    "            json_data = [search for search in json_data if search[\"name\"] not in best_results_df[\"search_id\"].values]\n",
    "\n",
    "        # Add a progress bar for processing data\n",
    "        for search in tqdm(json_data, desc=\"Processing Searches\", unit=\"search\"):\n",
    "            processed_data = process_data([search])\n",
    "\n",
    "            # Append the processed data to the CSV file\n",
    "            append_to_csv(OUTPUT_CSV, processed_data, header=header)\n",
    "\n",
    "        # Update the start index for the next chunk\n",
    "        start_index += chunk_size\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
