{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing image files for 1105458\n",
    "Missing image files for 3120677\n",
    "Missing image files for 4001768\n",
    "Missing image files for 7422236\n",
    "Missing image files for 7708216\n",
    "Missing image files for 7853938\n",
    "Missing image files for 8320908\n",
    "Missing image files for 9203389\n",
    "Missing image files for 9229131\n",
    "Missing image files for 9617107\n",
    "Missing image files for 9989721\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--window_size=512 --overlap=32 --batch_size=4 --epochs_classifier=1 --epochs_finetuning=1 --annotations_path=\"../data/annotations_ukraine_n_20.csv\" --from_finetuning=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Constants\n",
    "model_size = 1.3 * (1024 ** 2)  # Model size in bytes\n",
    "input_size_per_image = 22 * (1024 ** 2)  # Size of one image in bytes\n",
    "feature_size = 50 * (1024 ** 2)  # Size of features in bytes\n",
    "available_memory = 24 * (1024 ** 3)  # Available GPU memory in bytes\n",
    "\n",
    "# Estimate the overhead (2-3 times the model and input size)\n",
    "overhead_factor = 3\n",
    "\n",
    "# Finding maximum batch size\n",
    "max_batch_size = 1\n",
    "while True:\n",
    "    input_size = max_batch_size * (input_size_per_image * 2)  # For two images\n",
    "    features_size = max_batch_size * feature_size  # If applicable\n",
    "    total_memory = (model_size + input_size + features_size) * overhead_factor\n",
    "    \n",
    "    if total_memory > available_memory:\n",
    "        break\n",
    "    max_batch_size += 1\n",
    "\n",
    "print(f'Maximum batch size: {max_batch_size - 1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model constants\n",
    "model_size_mb = 1.3 * 1024  # ViT-B MAE model size in MB\n",
    "feature_dim = 768  # ViT-B feature dimension \n",
    "window_size = 224\n",
    "overlap = 0\n",
    "patch_size = 16\n",
    "\n",
    "# Image parameters (typical satellite image)\n",
    "img_height = 2017 \n",
    "img_width = 2028\n",
    "img_channels = 3\n",
    "\n",
    "# Calculate windows per image\n",
    "stride = window_size - overlap\n",
    "windows_h = max(1, (img_height - window_size) // stride + 1)\n",
    "windows_w = max(1, (img_width - window_size) // stride + 1)\n",
    "total_windows = windows_h * windows_w\n",
    "\n",
    "print(f\"Image dimensions: {img_height}x{img_width}\")\n",
    "print(f\"Windows per dimension: {windows_h}x{windows_w} = {total_windows} total windows\")\n",
    "\n",
    "# Image memory requirements\n",
    "image_size_mb = 2 * img_height * img_width * img_channels * 4 / (1024*1024)  # Two images (before/after), 4 bytes per float\n",
    "\n",
    "# Window processing (peak during feature extraction)\n",
    "inference_window_batch_size = 32  # Adjust based on implementation\n",
    "peak_window_batch_mb = window_size * window_size * img_channels * 4 * inference_window_batch_size / (1024*1024)\n",
    "\n",
    "# Feature extraction memory\n",
    "feature_tokens_per_window = (window_size // patch_size) ** 2 + 1  # +1 for CLS token\n",
    "feature_size_per_window_mb = feature_tokens_per_window * feature_dim * 4 / (1024*1024)  # 4 bytes per float\n",
    "feature_map_mb = feature_size_per_window_mb * total_windows  # Dictionary of all window features\n",
    "\n",
    "# Final merged feature representation (only active tokens in circle)\n",
    "feature_grid_h = img_height // patch_size\n",
    "feature_grid_w = img_width // patch_size\n",
    "total_grid_size = feature_grid_h * feature_grid_w\n",
    "circle_ratio = 3.14159 / 4  # π/4 (ratio of circle to square)\n",
    "active_tokens = int(total_grid_size * circle_ratio) + 1  # +1 for CLS\n",
    "final_feature_mb = active_tokens * feature_dim * 4 / (1024*1024)\n",
    "\n",
    "# Total memory per sample (accounting for both before/after images)\n",
    "# During processing we need: raw images + peak of either window batch or feature map + final representation\n",
    "processing_memory_mb = image_size_mb + max(peak_window_batch_mb, feature_map_mb) + 2 * final_feature_mb\n",
    "\n",
    "# Training specific memory (gradients, optimizer states)\n",
    "training_overhead_factor = 3  # Higher for training due to gradients\n",
    "training_memory_mb = processing_memory_mb * training_overhead_factor\n",
    "\n",
    "# Calculate max batch size based on available GPU memory\n",
    "available_memory_mb = 24 * 1024  # 24 GB GPU\n",
    "max_batch_size = max(1, int((available_memory_mb - model_size_mb) / training_memory_mb))\n",
    "\n",
    "# Consider gradient accumulation\n",
    "gradient_accumulation_steps = 4\n",
    "effective_batch_size = max_batch_size * gradient_accumulation_steps\n",
    "\n",
    "print(f\"\\nMemory requirements:\")\n",
    "print(f\"  Raw images: {image_size_mb:.2f} MB\")\n",
    "print(f\"  Feature map dictionary: {feature_map_mb:.2f} MB\")\n",
    "print(f\"  Peak window batch: {peak_window_batch_mb:.2f} MB\")\n",
    "print(f\"  Final feature representation: {2 * final_feature_mb:.2f} MB (both images)\")\n",
    "print(f\"  Total processing memory: {processing_memory_mb:.2f} MB\")\n",
    "print(f\"  Total training memory: {training_memory_mb:.2f} MB (with gradients)\")\n",
    "print(f\"\\nBatch size calculations:\")\n",
    "print(f\"  Model size: {model_size_mb:.2f} MB\")\n",
    "print(f\"  Available GPU memory: {available_memory_mb:.2f} MB\")\n",
    "print(f\"  Maximum batch size: {max_batch_size}\")\n",
    "print(f\"  Effective batch size with gradient accumulation: {effective_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_per_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H=2017\n",
    "W=2028\n",
    "window_size=16\n",
    "window_n1 = 0\n",
    "h_center, w_center = H/2, W/2\n",
    "\n",
    "for h in range(0, H - window_size + 1, window_size):\n",
    "        for w in range(0, W - window_size + 1, window_size):\n",
    "            window_n1 += 1\n",
    "window_n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate circle parameters\n",
    "H=2017\n",
    "W=2028\n",
    "window_size=16\n",
    "window_n2 = 0\n",
    "h_center, w_center = H/2, W/2\n",
    "radius = min(H, W)/2 - 1  # Slightly smaller than half the image dimension\n",
    "\n",
    "# Create batches of windows, but only include windows that intersect with the circle\n",
    "for h in range(0, H - window_size + 1, window_size):\n",
    "    for w in range(0, W - window_size + 1, window_size):\n",
    "        # Calculate window center\n",
    "        window_center_h = h + window_size/2\n",
    "        window_center_w = w + window_size/2\n",
    "        \n",
    "        # Calculate distance from window center to image center\n",
    "        dist_to_center = ((window_center_h - h_center)**2 + (window_center_w - w_center)**2)**0.5\n",
    "        \n",
    "        # Only process windows that are at least partially within the circle\n",
    "        window_radius = window_size/2 * 1.414  # Diagonal radius of window (√2 × half_size)\n",
    "        if dist_to_center <= radius + window_radius:\n",
    "            window_n2 += 1\n",
    "window_n2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_n2/window_n1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
