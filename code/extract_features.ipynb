{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am loading the mae_vit_base_patch16_dec512d8b model with pretrained weights. I am trying to then use this to extract features from my images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import rasterio\n",
    "\n",
    "# Import your custom modules\n",
    "from vitae_models.models_mae import mae_vit_base_patch16_dec512d8b\n",
    "\n",
    "from cd_dataset import ChangeDetectionDataset\n",
    "\n",
    "def load_model(checkpoint_path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Load the pretrained model from checkpoint\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_path: Path to the model checkpoint\n",
    "        device: Device to load the model on\n",
    "        \n",
    "    Returns:\n",
    "        Loaded model\n",
    "    \"\"\"\n",
    "    model = mae_vit_base_patch16_dec512d8b()\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_4band_image(path):\n",
    "    \"\"\"Load a 4-band image and ensure it has dimensions [1, channels, height, width]\"\"\"\n",
    "    with rasterio.open(path) as src:\n",
    "        # Read the bands (assuming band 1 is infrared, band 2 is red, band 3 is green, band 4 is blue)\n",
    "        blue = src.read(1)\n",
    "        green = src.read(2)\n",
    "        red = src.read(3)\n",
    "\n",
    "        # Stack the bands into a single array\n",
    "        img = np.stack((red, green, blue), axis=0)  # [channels, height, width]\n",
    "\n",
    "        # Normalize the image\n",
    "        mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
    "        std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
    "        img = (img / 255.0 - mean) / std\n",
    "\n",
    "        # Convert to tensor and add batch dimension\n",
    "        img = torch.from_numpy(img).float()\n",
    "        img = img.unsqueeze(0)  # [1, channels, height, width]\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def extract_features(model, image, device='cuda', batch_size=1):\n",
    "    \"\"\"\n",
    "    Extract features from an image using the pretrained model\n",
    "    \n",
    "    Args:\n",
    "        model: Pretrained model\n",
    "        image: Input tensor of shape [C, H, W]\n",
    "        device: Device to run inference on\n",
    "        batch_size: Batch size for processing\n",
    "        \n",
    "    Returns:\n",
    "        Extracted features\n",
    "    \"\"\"\n",
    "    # Ensure image has batch dimension\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.unsqueeze(0)  # [1, C, H, W]\n",
    "        \n",
    "    # Move to device\n",
    "    image = image.to(device)\n",
    "    \n",
    "    # Extract features without computing gradients\n",
    "    with torch.no_grad():\n",
    "        features, _, _ = model.forward_encoder(image, mask_ratio=0.0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "def process_large_image_efficiently(model, image, window_size=224, overlap=56, device='cuda', batch_size=4):\n",
    "    \"\"\"\n",
    "    Process large images using sliding windows with batched processing\n",
    "    \n",
    "    Args:\n",
    "        model: Pretrained model\n",
    "        image: Input tensor of shape [C, H, W] or [1, C, H, W]\n",
    "        window_size: Size of sliding window\n",
    "        overlap: Overlap between windows\n",
    "        device: Device to run inference on\n",
    "        batch_size: Batch size for processing windows\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping window positions to features\n",
    "    \"\"\"\n",
    "    # Ensure image has batch dimension\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.unsqueeze(0)  # [1, C, H, W]\n",
    "    \n",
    "    _, C, H, W = image.shape\n",
    "    stride = window_size - overlap\n",
    "    \n",
    "    # Calculate number of windows\n",
    "    n_windows_h = max(1, (H - window_size + stride) // stride)\n",
    "    n_windows_w = max(1, (W - window_size + stride) // stride)\n",
    "    total_windows = n_windows_h * n_windows_w\n",
    "    \n",
    "    # Create batches of windows for efficient processing\n",
    "    window_positions = []\n",
    "    window_batches = []\n",
    "    current_batch = []\n",
    "    \n",
    "    for h in range(0, H - window_size + 1, stride):\n",
    "        for w in range(0, W - window_size + 1, stride):\n",
    "            patch = image[:, :, h:h+window_size, w:w+window_size]\n",
    "            current_batch.append(patch)\n",
    "            window_positions.append((h, w))\n",
    "            \n",
    "            # Process batch when it reaches the desired size\n",
    "            if len(current_batch) == batch_size:\n",
    "                window_batches.append(torch.cat(current_batch, dim=0))\n",
    "                current_batch = []\n",
    "    \n",
    "    # Handle the last batch if it's not full\n",
    "    if current_batch:\n",
    "        window_batches.append(torch.cat(current_batch, dim=0))\n",
    "    \n",
    "    # Process batches and extract features\n",
    "    feature_map = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(window_batches):\n",
    "            # Process batch\n",
    "            batch = batch.to(device)\n",
    "            features, _, _ = model.forward_encoder(batch, mask_ratio=0.0)\n",
    "            \n",
    "            # Assign features to their respective positions\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(window_positions))\n",
    "            \n",
    "            for i, position_idx in enumerate(range(start_idx, end_idx)):\n",
    "                h, w = window_positions[position_idx]\n",
    "                feature_map[(h, w)] = features[i:i+1]  # Keep batch dimension\n",
    "            \n",
    "            # Free up memory\n",
    "            del features\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    return feature_map\n",
    "\n",
    "def merge_feature_map(feature_map, image_shape, window_size=224, overlap=56):\n",
    "    \"\"\"\n",
    "    Merge overlapping feature patches while preserving token structure\n",
    "    \n",
    "    Args:\n",
    "        feature_map: Dictionary mapping positions to features\n",
    "        image_shape: Original image shape (H, W)\n",
    "        window_size: Window size used for extraction\n",
    "        overlap: Overlap used between windows\n",
    "        \n",
    "    Returns:\n",
    "        Merged features tensor with CLS token preserved at position 0\n",
    "    \"\"\"\n",
    "    H, W = image_shape\n",
    "    patch_size = 16  # ViT-B uses 16Ã—16 patches\n",
    "    \n",
    "    # Calculate the feature map dimensions (downsampled by patch_size)\n",
    "    feature_h = H // patch_size\n",
    "    feature_w = W // patch_size\n",
    "    feature_dim = next(iter(feature_map.values())).shape[-1]\n",
    "    \n",
    "    # Initialize the merged feature map and a count map for averaging\n",
    "    merged = torch.zeros((1, feature_h, feature_w, feature_dim), device=next(iter(feature_map.values())).device)\n",
    "    counts = torch.zeros((1, feature_h, feature_w, 1), device=next(iter(feature_map.values())).device)\n",
    "    \n",
    "    # Collect all CLS tokens\n",
    "    cls_tokens = []\n",
    "    \n",
    "    # Calculate stride in token space\n",
    "    stride = (window_size - overlap) // patch_size\n",
    "    \n",
    "    # Add each feature patch to the appropriate position\n",
    "    for (h, w), feat in feature_map.items():\n",
    "        # Save the CLS token (first token)\n",
    "        cls_tokens.append(feat[:, 0, :])\n",
    "        \n",
    "        # Calculate token position\n",
    "        h_token = h // patch_size\n",
    "        w_token = w // patch_size\n",
    "        \n",
    "        # Get the actual expected number of tokens based on the window size\n",
    "        tokens_per_side = window_size // patch_size\n",
    "        \n",
    "        # Process patch tokens (skip CLS token)\n",
    "        feature_tokens = feat[:, 1:, :]\n",
    "        B, L, D = feature_tokens.shape\n",
    "        \n",
    "        # Check if L matches expected token count\n",
    "        expected_L = tokens_per_side * tokens_per_side\n",
    "        if L != expected_L:\n",
    "            print(f\"Warning: Feature shape mismatch. Expected {expected_L}, got {L}\")\n",
    "            continue\n",
    "            \n",
    "        feat_reshaped = feature_tokens.reshape(B, tokens_per_side, tokens_per_side, D)\n",
    "        \n",
    "        # Calculate boundaries making sure we don't go out of bounds\n",
    "        h_end = min(h_token + tokens_per_side, feature_h)\n",
    "        w_end = min(w_token + tokens_per_side, feature_w)\n",
    "        \n",
    "        # Size of patch that fits in the feature map\n",
    "        h_size = h_end - h_token\n",
    "        w_size = w_end - w_token\n",
    "        \n",
    "        merged[:, h_token:h_end, w_token:w_end] += feat_reshaped[:, :h_size, :w_size]\n",
    "        counts[:, h_token:h_end, w_token:w_end] += 1\n",
    "    \n",
    "    # Average overlapping regions (avoid division by zero)\n",
    "    merged = merged / (counts + 1e-8)\n",
    "    \n",
    "    # Average all collected CLS tokens\n",
    "    avg_cls_token = torch.mean(torch.cat(cls_tokens, dim=0), dim=0, keepdim=True).unsqueeze(0)\n",
    "    \n",
    "    # Reshape merged features from [B, H, W, D] to [B, L, D] format\n",
    "    B, H, W, D = merged.shape\n",
    "    merged_flat = merged.reshape(B, H*W, D)\n",
    "    \n",
    "    # Concatenate CLS token at position 0\n",
    "    merged_with_cls = torch.cat([avg_cls_token, merged_flat], dim=1)\n",
    "    \n",
    "    return merged_with_cls\n",
    "\n",
    "\n",
    "# Function to get a sample by timeline_id\n",
    "def get_sample_by_timeline_id(dataset, timeline_id):\n",
    "    # Find the index of the timeline_id in the dataset\n",
    "    try:\n",
    "        idx = dataset.names.index(timeline_id)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"timeline_id {timeline_id} not found in the dataset.\")\n",
    "    \n",
    "    # Use __getitem__ to retrieve the sample\n",
    "    sample = dataset[idx]\n",
    "    return sample\n",
    "\n",
    "def extract_and_save_features(image_id, before_dir, after_dir, \n",
    "                              output_dir, model, device='cuda', use_merged=True):\n",
    "    \"\"\"\n",
    "    Extract features for a single image pair and save them\n",
    "    \n",
    "    Args:\n",
    "        image_id: ID of the image to process\n",
    "        before_dir: Directory containing 'before' images\n",
    "        after_dir: Directory containing 'after' images\n",
    "        output_dir: Directory to save features\n",
    "        device: Device to run on\n",
    "        use_merged: Whether to use merged features or raw feature maps\n",
    "        \n",
    "    Returns:\n",
    "        Paths to the saved feature files\n",
    "    \"\"\"\n",
    "    # initialise dataset\n",
    "    dataset = ChangeDetectionDataset(path=\"../data/annotations_ukraine.csv\",\n",
    "                                     before_path=before_dir,\n",
    "                                     after_path=after_dir,\n",
    "                                     stride=1,\n",
    "                                     transform=None,\n",
    "                                     normalise=True)\n",
    "\n",
    "    # Create output directories\n",
    "    before_out_dir = os.path.join(output_dir, 'before')\n",
    "    after_out_dir = os.path.join(output_dir, 'after')\n",
    "    Path(before_out_dir).mkdir(exist_ok=True, parents=True)\n",
    "    Path(after_out_dir).mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Build paths\n",
    "    before_path = os.path.join(before_dir, str(image_id), 'files', 'composite.tif')\n",
    "    after_path = os.path.join(after_dir, str(image_id), 'files', 'composite.tif')\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not os.path.exists(before_path) or not os.path.exists(after_path):\n",
    "        print(f\"Error: Files for image {image_id} not found\")\n",
    "        return False  # Indicate failure\n",
    "    \n",
    "    # Load images\n",
    "    sample = get_sample_by_timeline_id(dataset, image_id)\n",
    "    before_tensor, after_tensor, _ = sample[\"I1\"], sample[\"I2\"], sample[\"label\"]\n",
    "    \n",
    "    # Extract features\n",
    "    try:\n",
    "        # Process before image\n",
    "        before_feature_map = process_large_image_efficiently(\n",
    "            model, before_tensor, window_size=224, overlap=56, device=device, batch_size=4\n",
    "        )\n",
    "        \n",
    "        # Process after image\n",
    "        after_feature_map = process_large_image_efficiently(\n",
    "            model, after_tensor, window_size=224, overlap=56, device=device, batch_size=4\n",
    "        )\n",
    "        \n",
    "        # Decide whether to use raw feature maps or merged features\n",
    "        if use_merged:\n",
    "            before_features = merge_feature_map(before_feature_map, before_tensor.shape[2:], \n",
    "                                              window_size=224, overlap=56)\n",
    "            after_features = merge_feature_map(after_feature_map, after_tensor.shape[2:], \n",
    "                                             window_size=224, overlap=56)\n",
    "            \n",
    "            # Convert to CPU and numpy\n",
    "            before_features = before_features.cpu().numpy()\n",
    "            after_features = after_features.cpu().numpy()\n",
    "        else:\n",
    "            # Convert feature maps to CPU and numpy\n",
    "            before_features = {pos: feat.cpu().numpy() for pos, feat in before_feature_map.items()}\n",
    "            after_features = {pos: feat.cpu().numpy() for pos, feat in after_feature_map.items()}\n",
    "        \n",
    "        # Save features\n",
    "        before_output_path = os.path.join(before_out_dir, f\"{image_id}.npz\")\n",
    "        after_output_path = os.path.join(after_out_dir, f\"{image_id}.npz\")\n",
    "        \n",
    "        if use_merged:\n",
    "            np.savez_compressed(before_output_path, features=before_features)\n",
    "            np.savez_compressed(after_output_path, features=after_features)\n",
    "        else:\n",
    "            # Convert dictionary to lists for saving\n",
    "            positions = list(before_features.keys())\n",
    "            before_values = [before_features[pos] for pos in positions]\n",
    "            after_values = [after_features[pos] for pos in positions]\n",
    "            \n",
    "            # Save as numpy arrays\n",
    "            np.savez_compressed(before_output_path, \n",
    "                               positions=positions, \n",
    "                               features=before_values)\n",
    "            np.savez_compressed(after_output_path, \n",
    "                              positions=positions, \n",
    "                              features=after_values)\n",
    "        \n",
    "        return True  # Indicate success\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image {image_id}: {e}\")\n",
    "        return False  # Indicate failure\n",
    "    finally:\n",
    "        # Clean up\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def batch_extract_features(image_ids, before_dir, after_dir, checkpoint_path, \n",
    "                          output_dir, device='cuda', use_merged=True, skip_existing=True):\n",
    "    \"\"\"\n",
    "    Extract features for multiple image pairs\n",
    "    \n",
    "    Args:\n",
    "        image_ids: list of event ids to process\n",
    "        before_dir: Directory containing 'before' images\n",
    "        after_dir: Directory containing 'after' images\n",
    "        checkpoint_path: Path to model checkpoint\n",
    "        output_dir: Directory to save features\n",
    "        device: Device to run on\n",
    "        use_merged: Whether to use merged features or raw feature maps\n",
    "    \"\"\"\n",
    "    print(\"Loading model...\")\n",
    "    model = load_model(checkpoint_path, device)\n",
    "\n",
    "    # Process each image pair\n",
    "    success_count = 0\n",
    "    \n",
    "    for image_id in tqdm(image_ids, desc=\"Extracting features\"):\n",
    "        if skip_existing:\n",
    "            before_output_path = os.path.join(output_dir, 'before', f\"{image_id}.npz\")\n",
    "            after_output_path = os.path.join(output_dir, 'after', f\"{image_id}.npz\")\n",
    "            \n",
    "            if os.path.exists(before_output_path) and os.path.exists(after_output_path):\n",
    "                print(f\"Skipping existing features for {image_id}\")\n",
    "                continue\n",
    "            \n",
    "        success = extract_and_save_features(\n",
    "            image_id, before_dir, after_dir, output_dir, \n",
    "            model, device=device, use_merged=use_merged\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            success_count += 1\n",
    "    \n",
    "    print(f\"Successfully extracted features for {success_count} out of {len(image_ids)} image pairs.\")\n",
    "\n",
    "def load_features(image_id, features_dir, subset='before', use_merged=True):\n",
    "    \"\"\"\n",
    "    Load extracted features for a specific image\n",
    "    \n",
    "    Args:\n",
    "        image_id: ID of the image\n",
    "        features_dir: Base directory containing features\n",
    "        subset: 'before' or 'after'\n",
    "        use_merged: Whether the features are merged or raw feature maps\n",
    "        \n",
    "    Returns:\n",
    "        Loaded features\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(features_dir, subset, f\"{image_id}.npz\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: Features for image {image_id} not found\")\n",
    "        return None\n",
    "    \n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "    if use_merged:\n",
    "        return data['features']\n",
    "    else:\n",
    "        # Reconstruct dictionary\n",
    "        positions = data['positions']\n",
    "        features = data['features']\n",
    "        \n",
    "        return {pos: feat for pos, feat in zip(positions, features)}\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description=\"Extract features for change detection\")\n",
    "#     parser.add_argument(\"--image_ids\", type=list, required=True, help=\"list of image pair ids\")\n",
    "#     parser.add_argument(\"--before_dir\", type=str, required=True, help=\"Directory with 'before' images\")\n",
    "#     parser.add_argument(\"--after_dir\", type=str, required=True, help=\"Directory with 'after' images\")\n",
    "#     parser.add_argument(\"--checkpoint\", type=str, required=True, help=\"Path to model checkpoint\")\n",
    "#     parser.add_argument(\"--output_dir\", type=str, required=True, help=\"Directory to save features\")\n",
    "#     parser.add_argument(\"--device\", type=str, default=\"cuda\", help=\"Device to run on\")\n",
    "#     parser.add_argument(\"--use_merged\", action=\"store_true\", help=\"Use merged features instead of raw feature maps\")\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     batch_extract_features(\n",
    "#         args.image_ids, \n",
    "#         args.before_dir, \n",
    "#         args.after_dir, \n",
    "#         args.checkpoint, \n",
    "#         args.output_dir,\n",
    "#         args.device,\n",
    "#         args.use_merged\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(image_id, features_dir, subset='before', use_merged=True):\n",
    "    \"\"\"\n",
    "    Load extracted features for a specific image\n",
    "    \n",
    "    Args:\n",
    "        image_id: ID of the image\n",
    "        features_dir: Base directory containing features\n",
    "        subset: 'before' or 'after'\n",
    "        use_merged: Whether the features are merged or raw feature maps\n",
    "        \n",
    "    Returns:\n",
    "        Loaded features\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(features_dir, subset, f\"{image_id}.npz\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: Features for image {image_id} not found\")\n",
    "        return None\n",
    "    \n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_extract_features(image_ids=[10014382, 9993914], before_dir=\"../data/images_ukraine_extracted_before\",\n",
    "              after_dir=\"../data/images_ukraine_extracted_after\", checkpoint_path=\"../data/model_weights/vit-b-checkpoint-1599.pth\", \n",
    "              output_dir=\"../data/features\", device='cuda', use_merged=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
