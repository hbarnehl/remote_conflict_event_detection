{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from planet_helpers import load_search_files\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "SEARCH_DIR = DATA_DIR + \"/searches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline_df = pd.read_csv(DATA_DIR + \"/ACLED_Ukraine_events_timeline.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "randint = random.randint(0, 130000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "searches = load_search_files(SEARCH_DIR, num_files=1, start_index=randint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches[\"results\"][0][\"geometry\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searches[\"filter\"][\"config\"][0][\"config\"][\"coordinates\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(file_path, data, header=None):\n",
    "    file_exists = os.path.isfile(file_path)\n",
    "    with open(file_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if header and not file_exists:\n",
    "            writer.writerow(header)\n",
    "        writer.writerows(data)\n",
    "\n",
    "def find_best(results, event_date):\n",
    "    # for the set of days before/after the event date:\n",
    "        # for each day where there are images\n",
    "            # combine all images and calculate overlap with area of interest\n",
    "            # if overlap is above threshold:\n",
    "                # find all combinations of images that produce overlap above threshold\n",
    "                # for each combination in this set:\n",
    "                    # calculate weighted average of clear percentage/cloud cover\n",
    "                # return the combination with the highest weighted average\n",
    "        # return the best combination for all days\n",
    "\n",
    "def process_data(data):\n",
    "    # Placeholder for data processing logic\n",
    "    # This function should return a list of processed data rows\n",
    "    processed_data = []\n",
    "    for search in data:\n",
    "        search_name = search[\"name\"]\n",
    "        search_id = search[\"id\"]\n",
    "        day_minus_5 = datetime.strptime(searches[\"filter\"][\"config\"][1][\"config\"][\"gt\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        event_date = (day_minus_5 + timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "        quality_results = [result for result in search[\"results\"] if result[\"properties\"][\"quality_category\"] == \"standard\"]\n",
    "        best_before, best_after = find_best(quality_results, event_date)\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create best matches for each search with columns search_id, before_image_id, before_cloud_cover/clarity etc., after_image_id, after_cloud_cover\n",
    "\n",
    "start_index = 0\n",
    "chunk_size = 10000\n",
    "\n",
    "while True:\n",
    "    # Load a chunk of files\n",
    "    json_data = load_search_files(SEARCH_DIR, chunk_size, start_index=start_index)\n",
    "    if not json_data:\n",
    "        break\n",
    "\n",
    "    # Process the loaded data\n",
    "    processed_data = process_data(json_data)\n",
    "\n",
    "    # Append the processed data to the CSV file\n",
    "    append_to_csv(OUTPUT_CSV, processed_data, header=header)\n",
    "\n",
    "    # Update the start index for the next chunk\n",
    "    start_index += chunk_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
