{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import getpass\n",
    "from planet import Auth, Session\n",
    "import json\n",
    "from planet_helpers import set_filters, parse_polygon, load_search_files\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "DOWNLOAD_DIR = '../data/planet_data'\n",
    "FILTER_DIR = '../data/filters'\n",
    "SEARCH_DIR = \"../data/searches_alt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = input(\"Username: \")\n",
    "pw = getpass.getpass()\n",
    "auth = Auth.from_login(user,pw)\n",
    "auth.store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = pd.read_csv('../data/ACLED_Ukraine_events_sample.csv')[['location_id', 'event_date', 'event_id_cnty', 'timeline_id']]\n",
    "non_events = pd.read_csv('../data/ACLED_Ukraine_non_events_sample.csv')[['location_id', 'event_date', 'timeline_id']]\n",
    "\n",
    "places = pd.read_csv(\"../data/places.csv\")\n",
    "\n",
    "# merge events with places on location_id\n",
    "events = events.merge(places, on='location_id')\n",
    "non_events = non_events.merge(places, on='location_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filters = []\n",
    "\n",
    "for event in tqdm(non_events.iterrows(), total=len(non_events)):\n",
    "   \n",
    "   coords = parse_polygon(event[1]['geometry'])\n",
    "   \n",
    "   geom = {\n",
    "         \"type\": \"Polygon\",\n",
    "         \"coordinates\": coords\n",
    "      }\n",
    "\n",
    "   date = datetime.strptime(event[1][\"event_date\"], \"%Y-%m-%d\")\n",
    "   # subtract 5 days from the event date\n",
    "   five_before = (date - pd.DateOffset(days=5)).to_pydatetime()\n",
    "   five_after = (date + pd.DateOffset(days=5)).to_pydatetime()\n",
    "\n",
    "   filters = set_filters(from_date=five_before, to_date = five_after, geom=geom)\n",
    "   all_filters.append(filters)\n",
    "\n",
    "# save filters\n",
    "with open(FILTER_DIR + '/filters_non_events.jsonl', 'w') as file:\n",
    "    for entry in all_filters:\n",
    "        # Convert each dictionary to a JSON string and write it to the file\n",
    "        file.write(json.dumps(entry) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the filter to searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [\"non_events\", \"events\"]:\n",
    "    event_type = x\n",
    "    df = pd.read_csv('../data/ACLED_Ukraine_' + event_type + '_sample.csv')\n",
    "\n",
    "    async with Session() as sess:\n",
    "        cl = sess.client('data')\n",
    "\n",
    "        with open(FILTER_DIR + '/filters_' + event_type + '.jsonl', 'r') as file:\n",
    "            # Initialize tqdm with manual update\n",
    "            pbar = tqdm(total=len(df))\n",
    "            \n",
    "            for i, line in enumerate(file):\n",
    "                timeline_id = str(df['timeline_id'][i])\n",
    "                name = event_type + '_' + timeline_id\n",
    "                JSON_DIR = SEARCH_DIR + '/' + name + '.json'\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Check if file already exists\n",
    "                if os.path.exists(JSON_DIR):\n",
    "                    # print(f\"Search {name} already exists\")\n",
    "                    continue\n",
    "                else:\n",
    "                    # print(f\"Creating search {name}\")\n",
    "                    filters = json.loads(line)\n",
    "                    \n",
    "                    request = await cl.create_search(name=name,\n",
    "                                                    search_filter=filters,\n",
    "                                                    item_types=[\"PSScene\"])\n",
    "                    with open(JSON_DIR, 'w') as f:\n",
    "                        f.write(json.dumps(request))\n",
    "            \n",
    "            # Close the progress bar\n",
    "            pbar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiofiles import open as aio_open\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "\n",
    "async def process_file(dir, cl, pbar):\n",
    "    file_path = os.path.join(SEARCH_DIR, dir)\n",
    "\n",
    "    async with aio_open(file_path, \"r+\") as f:\n",
    "        # Load the existing JSON data\n",
    "        search = json.loads(await f.read())\n",
    "        pbar.update(1)\n",
    "        # if search has already been run, skip\n",
    "        if \"results\" in search.keys():\n",
    "            return\n",
    "\n",
    "        # Run the search and collect results\n",
    "        items = cl.run_search(search_id=search['id'], limit=100)\n",
    "        item_list = [i async for i in items]\n",
    "\n",
    "        # Update the search dictionary with results\n",
    "        search[\"results\"] = item_list\n",
    "\n",
    "        # Move the file pointer to the beginning\n",
    "        await f.seek(0)\n",
    "\n",
    "        # Write the updated JSON data\n",
    "        await f.write(json.dumps(search, indent=4))\n",
    "\n",
    "        # Truncate the file to remove any leftover data\n",
    "        await f.truncate()\n",
    "\n",
    "async def process_files(files, cl):\n",
    "    pbar = tqdm(total=len(files))\n",
    "    \n",
    "    # Process files concurrently\n",
    "    await asyncio.gather(*(process_file(dir, cl, pbar) for dir in files))\n",
    "    \n",
    "    # Close the progress bar\n",
    "    pbar.close()\n",
    "\n",
    "def chunk_files(file_list, chunk_size):\n",
    "    for i in range(0, len(file_list), chunk_size):\n",
    "        yield file_list[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = os.listdir(SEARCH_DIR)\n",
    "\n",
    "async with Session() as sess:\n",
    "    cl = sess.client('data')\n",
    "\n",
    "    for file_chunk in chunk_files(all_files, 2):\n",
    "        asyncio.run(process_files(file_chunk, cl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the first ten files in SEARCH_DIR into \"../data/searches_alt\"\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "files = os.listdir(SEARCH_DIR)[:10]\n",
    "for f in files:\n",
    "    shutil.copy(SEARCH_DIR + '/' + f, \"../data/searches_alt/\" + f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print time for each item\n",
    "for item in item_list:\n",
    "    if item['properties']['quality_category'] != 'test':\n",
    "        print(f\"acquired: {item['properties']['acquired']}, clear percent: {item['properties']['clear_percent']}, cloud percent: {item['properties']['cloud_percent']}, heavy haze percent: {item['properties']['heavy_haze_percent']}, light haze percent: {item['properties']['light_haze_percent']}, snow ice percent: {item['properties']['snow_ice_percent']}, shadow percent: {item['properties']['shadow_percent']}, visible percent: {item['properties']['visible_percent']}, visible confidence percent: {item['properties']['visible_confidence_percent']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seminar_paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
